{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_data_formatter.cleansing.model import PromptTextGenModel\n",
    "from ai_data_formatter.config import ModelConfig, DBClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy.engine import URL\n",
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "with open(\"set-env\") as stream:\n",
    "    config.read_string(\"[DEFAULT]\\n\" + stream.read())  # This line does the trick.\n",
    "\n",
    "with open(\"cleansing_prompt_template.json\", \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "\n",
    "pg_host=config['DEFAULT'].get(\"PG_HOST\")\n",
    "pg_uname=config['DEFAULT'].get(\"PG_UNAME\")\n",
    "pg_secret=config['DEFAULT'].get(\"PG_SECRET\")\n",
    "pg_db=config['DEFAULT'].get(\"PG_DB\")\n",
    "conn_str_alchemy = f\"postgresql://{pg_uname}:{pg_secret}@{pg_host}/{pg_db}\"\n",
    "cache_secret=config['DEFAULT'].get(\"CACHE_SECRET\")\n",
    "cache_host=config['DEFAULT'].get(\"CACHE_HOST\")\n",
    "cache_port=config['DEFAULT'].get(\"CACHE_PORT\")\n",
    "conn_str_redis = f\"redis://:{cache_secret}@{cache_host}:{cache_port}/0\"\n",
    "test_data = pd.read_csv(\"data/pii_org.csv\")\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"docai-warehouse-demo\"\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/zjia/Workspace/gen-ai-data-transformer/sa_token.json\"\n",
    "\n",
    "dbclient = DBClient.from_dict(\n",
    "    {\n",
    "        \"db\": {\n",
    "            \"url\": conn_str_alchemy\n",
    "        },\n",
    "        \"cache\": {\n",
    "            # \"url\": conn_str_redis,\n",
    "            \"expire_time_second\": 120\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import math\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from ratelimiter import RateLimiter\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import findspark\n",
    "findspark.init(edit_rc=False)\n",
    "\n",
    "rate_limiter = RateLimiter(max_calls=50, period=60)\n",
    "config = SparkConf().setAll([('spark.sql.autoBroadcastJoinThreshold', '-1')])\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .appName('aidf-app') \\\n",
    "    .config(conf=config).getOrCreate()\n",
    "\n",
    "def clean(column, tag):\n",
    "    \"\"\"\n",
    "    Apply cleansing\n",
    "    \"\"\"\n",
    "    model_config_spec = list(filter(lambda config: config.get(\"tag\")==tag, model_configs))[0]\n",
    "    model_config = ModelConfig.from_dict(model_config_spec)\n",
    "    model = PromptTextGenModel(\n",
    "        project_id=\"docai-warehouse-demo\", \n",
    "        location=\"us-central1\",\n",
    "        model_config=model_config\n",
    "    )\n",
    "    if not spark:\n",
    "        ## Parallel in single server with multiprocessing\n",
    "        data = pd.read_csv(\"data/pii_org.csv\")\n",
    "        values = data[column].unique()\n",
    "\n",
    "        parallem = cpu_count() - 1\n",
    "        batch_size = round(math.ceil(len(values) / parallem), 0)\n",
    "        batches = [values[(min((i)*batch_size, len(values)-1)):min((i+1)*batch_size, len(values)-1)] for i in range(0, parallem)]\n",
    "        \n",
    "        values_cln = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for values in batches:\n",
    "                with rate_limiter:\n",
    "                    futures.append(\n",
    "                        executor.submit(\n",
    "                            # function name\n",
    "                            model.batch_predict,\n",
    "                            # parameters\n",
    "                            values\n",
    "                        )\n",
    "                    )\n",
    "            for f in tqdm(\n",
    "                as_completed(futures),\n",
    "                desc=f\"Predicting the model output\",\n",
    "                total=len(futures),\n",
    "            ):\n",
    "                values_cln.append(list(f.result()))\n",
    "        return pd.DataFrame([item for sublist in values_cln for item in sublist], columns=[column, f\"{column}_cln\"])\n",
    "    \n",
    "    else:\n",
    "        from pyspark.sql.functions import col, pandas_udf\n",
    "        from pyspark.sql.types import ArrayType, StringType\n",
    "        ## Parallel in cluster using Apache Spark\n",
    "        data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/pii_org.csv\")\n",
    "        \n",
    "        @pandas_udf(ArrayType(StringType()))\n",
    "        def cln_udf(inputs):\n",
    "            return pd.Series(model.batch_predict(inputs))\n",
    "        \n",
    "        data = data.withColumn(f\"{column}_cln\", cln_udf(col(column)))\n",
    "        return data.select(f\"{column}_cln\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting the model output: 100%|██████████| 7/7 [00:02<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "spark = None\n",
    "result = clean(\"r_org\", \"cleansing-race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_dl\n",
      "-1    0.762152\n",
      " 0    0.762609\n",
      " 1    0.858779\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def evaluate(pii):\n",
    "    evaluation_data = test_data.merge(result, on=f\"{pii}_org\")\n",
    "    print(evaluation_data.groupby(f\"{pii}_dl\").apply(lambda x: sum(x[pii]==x[f\"{pii}_org_cln\"])/x[\"id\"].count()))\n",
    "evaluate(\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.to_csv(\"fn_org_cln.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
